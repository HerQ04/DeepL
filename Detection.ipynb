{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/HerQTheToxic/DeepL/blob/main/Detection.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "FcQ8O4-SnSyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Image path\n",
        "imagePath=\"/content/Test.png\""
      ],
      "metadata": {
        "id": "J2GYcXDUIGQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxldPEZMFjpT",
        "outputId": "befd4f75-dd3b-4193-b1f4-37434cd8fc95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 16078, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 16078 (delta 6), reused 9 (delta 1), pack-reused 16056\u001b[K\n",
            "Receiving objects: 100% (16078/16078), 14.64 MiB | 14.67 MiB/s, done.\n",
            "Resolving deltas: 100% (11038/11038), done.\n",
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.4/586.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.9/645.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.2/512.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt comet_ml  # install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from io import StringIO"
      ],
      "metadata": {
        "id": "Xm7r30k0E42Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Downloading trained model weights (stored in my public drive)\n",
        "!gdown https://drive.google.com/uc?id=1ocRpJD_TTCloMzQJ8NO3qCG16FphLSCO -O /content/Number.pt\n",
        "!gdown https://drive.google.com/uc?id=1BpJhTWsGL6ZvtPrDqbGiYV4DJXMW0QoF -O /content/Gem2.pt\n",
        "!gdown https://drive.google.com/uc?id=1jmatSFxw_TgjMyowckzvDAKBx50HPNOz -O /content/GemCls.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jitGCvppVDxE",
        "outputId": "05153883-00d8-4da3-889b-518ae6e6c775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ocRpJD_TTCloMzQJ8NO3qCG16FphLSCO\n",
            "To: /content/Number.pt\n",
            "100% 42.3M/42.3M [00:01<00:00, 29.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1BpJhTWsGL6ZvtPrDqbGiYV4DJXMW0QoF\n",
            "To: /content/Gem2.pt\n",
            "100% 42.3M/42.3M [00:01<00:00, 36.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jmatSFxw_TgjMyowckzvDAKBx50HPNOz\n",
            "To: /content/GemCls.pt\n",
            "100% 8.46M/8.46M [00:00<00:00, 73.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EWym17P7nRcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOLOv5 framework path\n",
        "yolov5Path = \"/content/yolov5\"\n",
        "\n",
        "#modell weight paths\n",
        "NumberModelPath=\"/content/Number.pt\"\n",
        "GemModelPath=\"/content/Gem2.pt\"\n",
        "GemModelClassPath=\"/content/GemCls.pt\""
      ],
      "metadata": {
        "id": "fYPfLn5LCuf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Detecting with the modell\n",
        "def detectx(frame, model):\n",
        "    results = model([frame])\n",
        "    labels, coordinates = results.pred[0][:, -1], results.pred[0][:, :-1]\n",
        "    return labels, coordinates"
      ],
      "metadata": {
        "id": "rf3Ob1ArH5Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading model with the trained weight\n",
        "def loadModel(path):\n",
        "    model = torch.hub.load(yolov5Path, 'custom', source='local', path=path, force_reload=True, skip_validation=True)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Zs6w4Ss95WPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Converting image into useable frame for detection\n",
        "def getFrame(image_path):\n",
        "    frame = cv2.imread(image_path)\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    return frame"
      ],
      "metadata": {
        "id": "jnoNspMk_uFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the numbers in the picture. Sorting in order by X coordinate and converting to final number\n",
        "def getDetectedNumber(frame, model):\n",
        "        #Getting the numbers\n",
        "    detectedNumbers = []\n",
        "\n",
        "    labelsNum, coordinatesNum = detectx(frame, model)\n",
        "    class_names_Num = model.names\n",
        "\n",
        "    for i in range(len(labelsNum)):\n",
        "            label = labelsNum[i]\n",
        "            coordinate = coordinatesNum[i]\n",
        "            if coordinate[4] >= threshold:\n",
        "                class_label = class_names_Num[int(label)]\n",
        "                x1, y1, x2, y2 = coordinate[:4]\n",
        "                detectedNumbers.append((class_label, x1))\n",
        "    if len(detectedNumbers) < 1:\n",
        "        return 0\n",
        "    else:\n",
        "        detectedNumbers = sorted(detectedNumbers, key=lambda x: x[1])\n",
        "\n",
        "        result_number = int(''.join(map(str, [obj[0] for obj in detectedNumbers])))\n",
        "        result_number = int(result_number)\n",
        "\n",
        "        return result_number"
      ],
      "metadata": {
        "id": "lKBRITx6_S8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find a gemstone in the picture. Return: gemstone class, confidence and the stone itself cut out of the image to be given to a classifier later\n",
        "def getDetectedGemFrameAndPred(frame, model):\n",
        "    labelsGem, coordinatesGem = detectx(frame, model)\n",
        "    class_names_Gem = model.names\n",
        "    for i in range(len(labelsGem)):\n",
        "        label = labelsGem[i]\n",
        "        coordinate = coordinatesGem[i]\n",
        "        if coordinate[4] >= threshold:\n",
        "            class_label = class_names_Gem[int(label)]\n",
        "            x1, y1, x2, y2 = map(int,coordinate[:4])\n",
        "            detected_object = frameGem[y1:y2, x1:x2]\n",
        "\n",
        "            # save_path = f\"/content/{class_label}_{i}.png\"\n",
        "            # cv2.imwrite(save_path, cv2.cvtColor(detected_object, cv2.COLOR_RGB2BGR))\n",
        "        return detected_object, class_label ,coordinate[4]\n",
        "    return None, None, None"
      ],
      "metadata": {
        "id": "gv8RXe0-DXDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization for image classification\n",
        "IMAGENET_MEAN = 0.485, 0.456, 0.406\n",
        "IMAGENET_STD = 0.229, 0.224, 0.225\n",
        "\n",
        "#Image transform for model\n",
        "def classify_transforms(size=224):\n",
        "    return T.Compose([T.ToTensor(), T.Resize(size), T.CenterCrop(size), T.Normalize(IMAGENET_MEAN, IMAGENET_STD)])"
      ],
      "metadata": {
        "id": "NgV1Y_HSBy0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from torchvision import transforms as T\n",
        "\n",
        "#Classification of the given image pieces\n",
        "def GemClass(model, decObj):\n",
        "    transformations = classify_transforms()\n",
        "    converted_tensor = transformations(Image.fromarray(cv2.cvtColor(decObj, cv2.COLOR_BGR2RGB)))\n",
        "    converted_tensor = converted_tensor.unsqueeze(0)\n",
        "\n",
        "\n",
        "    results = model(converted_tensor)\n",
        "    pred = F.softmax(results, dim=1)\n",
        "\n",
        "    max_confidence, max_class_idx = torch.max(pred, dim=1)\n",
        "    predGem=model.names[max_class_idx.item()]\n",
        "    conf=max_confidence.item()\n",
        "\n",
        "    return predGem, conf"
      ],
      "metadata": {
        "id": "FdrNotEfLJyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "\n",
        "detectedGems=[]\n",
        "\n",
        "#Loading models\n",
        "try:\n",
        "    modelNumber=loadModel(NumberModelPath)\n",
        "    modelGem = loadModel(GemModelPath)\n",
        "    modelGemClass = loadModel(GemModelClassPath)\n",
        "except:\n",
        "    print('Error with loading models')\n",
        "\n",
        "#Setting treshold\n",
        "global threshold\n",
        "threshold=0.1\n",
        "\n",
        "#Reading images\n",
        "try:\n",
        "\n",
        "    frameNumber =getFrame(imagePath)\n",
        "    frameGem = getFrame(imagePath)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f'Error reading the image: {str(e)}')\n",
        "\n",
        "\n",
        "#Detected number\n",
        "result_number=getDetectedNumber(frameNumber,modelNumber)\n",
        "#Detected gem\n",
        "detected_Gem, predObj, confObj=getDetectedGemFrameAndPred(frameGem, modelGem)\n",
        "\n",
        "\n",
        "if detected_Gem is None:\n",
        "    clear_output()\n",
        "    print(f'there is no detected Gem on the picture')\n",
        "else:\n",
        "    #Using classification after object detection, to make better prediction\n",
        "    predClass,confCls =GemClass(modelGemClass, detected_Gem)\n",
        "    clear_output()\n",
        "    print(f'Gemstone\\'s weight: {result_number}, pred: {predObj}/{predClass}  confidence: {confObj}/{confCls}')\n",
        "\n"
      ],
      "metadata": {
        "id": "8tYWgZHqIiRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d44c256-62df-4d6c-d22f-aaee19c3d0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemstone's weight: 6789, pred: Emerald/Emerald  confidence: 0.8488202095031738/0.7797858119010925\n"
          ]
        }
      ]
    }
  ]
}